# Retail Sales Data Engineering Pipeline

This project automates a data engineering pipeline to analyze retail sales data and generate a comprehensive sales dashboard. This dashboard will empower us to visualize revenue generated by each store and identify the factors influencing those figures.
<div align="center">
<hr />

[Problem Statement](#problem-statement) •
[Solution](#solution) •
[Project Details](#project-details) •
[Data Pipeline Architecture](#data-pipeline-architecture) <br>
[Running the Project](#running-the-project) •
[Dashboard](#dashboard) • 
[Resources](#resources) • 
</div>
<hr/>

## Problem Statement

Gaining a clear understanding of store performance is crucial for effective retail management. However, manually analyzing vast sales data sets makes it difficult to:
- Compare revenue across stores: Quickly identify top-performing and underperforming locations.
- Uncover trends: Analyze factors impacting revenue, such as location, product mix, or marketing efforts.
- Make data-driven decisions: Effectively allocate resources and optimize strategies based on real-time insights.

## Solution

This pipeline automates data processing and prepares the data for analysis in Google BigQuery. This allows us to:
- Visualize store revenue: Create clear visualizations comparing revenue across all stores over time.
- Identify influencing factors: Analyze the impact of location, product categories, promotions, and other relevant factors on store performance.
- Gain actionable insights: Leverage the sales dashboard to pinpoint areas for improvement and make data-driven decisions to optimize store performance and maximize profitability.

## Project Details
This project proposes a data engineering pipeline that automates the data processing workflow utilizing the following tech stacks:
- [`Mage AI`](https://github.com/mage-ai/mage-ai) for data pipeline orchestration
- `Google Cloud Platform (GCP)` services for storage
- [`Pyspark`](https://github.com/apache/spark) for big data loading and transformation
- [`DBT`](https://github.com/dbt-labs/dbt-core) for data transformation 
- [`Terraform`](https://github.com/hashicorp/terraform) for cloud resources provision
- `Looker Studio` for dashboard

The pipeline will:

- Extract retail sales data from [Kaggle Retail Sales Data](https://www.kaggle.com/datasets/berkayalan/retail-sales-data/) which consists of sales data collected from a Turkish retail company. Time period begins from 2017 to the end of 2019.
- Validate the data against pre-defined schemas to ensure consistency and quality.
- Load the processed data into a BigQuery table.
- Transform the data to prepare it for analysis with DBT.

## Data Pipeline Architecture

Diagram below shows an overview of data pipeline architecture used in this project.

<p align="left">
    <img src="./screenshots/retail_sales_analysis_architecture.png" width="60%", height="60%"
    <em></em>
</p>

More info about data pipeline with Mage can be found [here](https://github.com/hwchua0209/Retail_Sales_Analysis/blob/main/retail_sales_analysis_mage/README.md).

## Running the Project

### Prerequisites
1. Clone this project to local drive. 
2. Ensure docker desktop is downloaded locally.
3. Ensure you have a GCP and Kaggle account.
4. Place the following JSON files to `secrets` folder. 
    - GCP Credentials API Key. Refer [here](https://cloud.google.com/docs/authentication/api-keys#create) for instructions. Ensure proper IAM role was given to the user.
    - Kaggle API Key. Refer [here](https://www.kaggle.com/docs/api) for instructions. 

### Makefile
This project utilizes a `Makefile` to automate various tasks. Here's how to use the Makefile to run the pipeline:

**1. Cloud resources provision:**
```script
make terraform
```
* Use the `make terraform` command to initialize Terraform in the terraform` directory. This make command will trigger a series of Terraform command to initialize Terraform in the Terraform directory, format Terraform code to ensure it adheres to a consistent style, preview the changes Terraform will make to your GCP resources based on your configuration and finally create or modify GCP resources as defined in your Terraform configuration.

**Remember:**

* Update the `terraform` directory with your GCP project ID and desired resource configurations before running these commands.

**2. Destroying Terraform resources:**
```script
make terraform_destroy
```
* Use `make terraform_destroy` to remove all resources provisioned by Terraform. This is useful for cleaning up your environment.

**3. Launching the Data Pipeline:**
```script
make start
```
* Run `make start` to start the data processing container defined in `docker-compose.yml`. This container executes the data pipeline steps using Mage AI.

**4. Stopping the Data Pipeline and clean up resources:**
```script
make clean
```
* Use `make clean` to stop the running data processing container and clean up stopped images, container and volume.

**5. Getting Help:**
```script
Available rules:

clean               Clean up data pipeline container
start               Launch data pipeline container
terraform           Create cloud resources via Terraform
terraform_destroy   Destroy cloud resources via Terraform
```
* Type `make` to display a list of available commands and their brief descriptions.

## Dashboard
<p align="left">
    <img src="./screenshots/Retail_Sales_Analysis_Dashboard.png" width="60%", height="60%"<br/>
    <em></em>
</p>

The dashboard is available for viewing [here](https://lookerstudio.google.com/reporting/303e81af-f3c3-4c08-90f0-85bcb9cbce85).

## Resources

- [Data Engineering Zoomcamp by DataTalks.Club](https://github.com/DataTalksClub/data-engineering-zoomcamp)
